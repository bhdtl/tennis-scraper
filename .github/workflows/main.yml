name: Neural Scout Sync V91.0 (API-First)

on:
  schedule:
    # Optimiert auf alle 4 Stunden, um das 500-Credits-Limit der API zu wahren.
    # Dies entspricht ca. 360 Requests pro Monat (ATP + WTA).
    - cron: '0 */4 * * *' 
  workflow_dispatch:        # Erlaubt den manuellen Start f√ºr Ad-hoc Analysen

# Best Practice: Minimale Rechte f√ºr maximale Sicherheit
permissions:
  contents: read

jobs:
  sync:
    name: Execute Data Ingestion Pipeline
    runs-on: ubuntu-latest
    timeout-minutes: 15     # Reduziert von 60, da die API viel schneller ist als Scraping

    steps:
      - name: üìÇ Checkout Repository Assets
        uses: actions/checkout@v4

      - name: üêç Initialize Python Runtime (3.10)
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'      # Automatisiertes Caching f√ºr schnellere Runs

      - name: üì¶ Install Core Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: üé≠ Install Playwright (for Elo-Ratings Enrichment)
        # Wir behalten Playwright nur f√ºr die Elo-Daten von Tennis Abstract
        run: |
          playwright install chromium --with-deps

      - name: üöÄ Run Neural Scout V91 Engine
        env:
          # SECURITY: Alle Keys m√ºssen in den GitHub Repository Settings hinterlegt sein
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          THE_ODDS_API_KEY: ${{ secrets.THE_ODDS_API_KEY }} # <--- ESSENZIELL F√úR V91
        run: |
          python scraper.py
